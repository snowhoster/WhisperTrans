# 翻譯連線測試疑難排解指南

## 問題描述

在 WhisperTrans 中測試翻譯連線時總是失敗，顯示「? 連線失敗」。

## 已修復的問題

### 1. API Key 驗證錯誤 ?

**問題：**
```csharp
// ? 錯誤代碼
if (string.IsNullOrWhiteSpace(_apiUrl) || string.IsNullOrWhiteSpace(_apiKey))
    throw new InvalidOperationException("翻譯服務尚未配置");
```

這會導致本地模型（vLLM、Ollama）因為沒有 API Key 而失敗。

**修復：**
```csharp
// ? 正確代碼
if (string.IsNullOrWhiteSpace(_apiUrl))
    throw new InvalidOperationException("翻譯服務尚未配置 API URL");
```

只檢查 API URL，API Key 是選填的。

### 2. 測試連線改進 ?

**新增：**
- 詳細的錯誤日誌
- 更好的錯誤訊息
- 調試輸出

## 診斷步驟

### 步驟 1: 檢查 Output 視窗

在 Visual Studio 中：
1. 按 `Ctrl + Alt + O` 開啟 Output 視窗
2. 選擇「Debug」選項
3. 查看連線測試的詳細日誌

您應該看到類似：
```
測試連線 - URL: http://localhost:8000/v1/chat/completions
測試連線 - 模型: Qwen/Qwen2-7B-Instruct
測試連線 - 提供商: vllm
```

### 步驟 2: 檢查設定

#### OpenAI
```
LLM 提供商: OpenAI (GPT)
API URL: https://api.openai.com/v1/chat/completions
API Key: sk-proj-xxxxxxxxxxxxx  ← 必填
模型名稱: gpt-3.5-turbo
```

#### vLLM (本地)
```
LLM 提供商: 本地模型 (vLLM)
API URL: http://localhost:8000/v1/chat/completions
API Key: (留空或任意文字)  ← 選填
模型名稱: Qwen/Qwen2-7B-Instruct
```

#### Ollama (本地)
```
LLM 提供商: 本地模型 (Ollama)
API URL: http://localhost:11434/api/chat
API Key: (留空)  ← 選填
模型名稱: llama2
```

### 步驟 3: 驗證 API URL

#### 測試 vLLM
```bash
# 在 PowerShell 或 cmd 中執行
curl http://localhost:8000/v1/models

# 應該返回類似：
{
  "object": "list",
  "data": [
    {
      "id": "Qwen/Qwen2-7B-Instruct",
      "object": "model",
      ...
    }
  ]
}
```

#### 測試 Ollama
```bash
curl http://localhost:11434/api/tags

# 應該返回已安裝的模型列表
```

#### 測試 OpenAI
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### 步驟 4: 檢查網路連線

#### 本地服務
```powershell
# 檢查端口是否開啟
Test-NetConnection -ComputerName localhost -Port 8000  # vLLM
Test-NetConnection -ComputerName localhost -Port 11434  # Ollama
```

#### 雲端服務
```powershell
# 檢查網路連線
Test-NetConnection -ComputerName api.openai.com -Port 443
```

## 常見錯誤與解決方案

### 錯誤 1: 連線被拒絕

**錯誤訊息：**
```
翻譯請求失敗: Connection refused
```

**原因：**
- vLLM/Ollama 伺服器未啟動
- 端口號錯誤
- 防火牆阻擋

**解決方法：**

1. 確認服務已啟動：
```bash
# vLLM
python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct

# Ollama
ollama serve
```

2. 檢查端口：
```powershell
netstat -an | findstr "8000"   # vLLM
netstat -an | findstr "11434"  # Ollama
```

3. 檢查防火牆：
```powershell
# Windows Firewall
netsh advfirewall firewall add rule name="vLLM" dir=in action=allow protocol=TCP localport=8000
```

### 錯誤 2: 401 Unauthorized

**錯誤訊息：**
```
翻譯請求失敗: 401 Unauthorized
```

**原因：**
- API Key 錯誤或過期
- API Key 格式不正確

**解決方法：**

1. 檢查 API Key 格式：
   - OpenAI: `sk-proj-xxxxxx`
   - Azure: 32 字元的密鑰

2. 驗證 API Key：
```bash
# OpenAI
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

3. 重新生成 API Key（如果需要）

### 錯誤 3: 404 Not Found

**錯誤訊息：**
```
翻譯請求失敗: 404 Not Found
```

**原因：**
- API URL 錯誤
- 模型名稱錯誤

**解決方法：**

1. 檢查 vLLM URL：
```
? 錯誤: http://localhost:8000/chat/completions
? 正確: http://localhost:8000/v1/chat/completions
```

2. 檢查模型名稱：
```bash
# 列出可用模型
curl http://localhost:8000/v1/models
```

3. 確認模型名稱與啟動時一致：
```bash
# 啟動時使用的模型名稱
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2-7B-Instruct

# WhisperTrans 中也要使用相同名稱
模型名稱: Qwen/Qwen2-7B-Instruct
```

### 錯誤 4: 超時

**錯誤訊息：**
```
翻譯請求失敗: The operation has timed out
```

**原因：**
- 伺服器回應太慢
- 網路延遲
- 模型加載中

**解決方法：**

1. 等待模型完全載入（首次啟動可能需要 1-2 分鐘）

2. 檢查伺服器日誌：
```
# vLLM 啟動後應該看到：
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000
```

3. 使用較小的模型（如果資源有限）

### 錯誤 5: 模型名稱不符

**錯誤訊息：**
```
解析翻譯結果失敗: ...
```

**原因：**
- 回應格式不符合 OpenAI API 標準
- 模型不支援對話格式

**解決方法：**

1. 確認使用 Instruct 版本的模型：
```
? 正確: Qwen/Qwen2-7B-Instruct
? 錯誤: Qwen/Qwen2-7B（基礎模型）
```

2. 檢查 API 回應格式：
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2-7B-Instruct",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

## 完整測試流程

### 1. vLLM 完整測試

```powershell
# 步驟 1: 啟動 vLLM
python -m vllm.entrypoints.openai.api_server `
    --model Qwen/Qwen2-7B-Instruct `
    --host 127.0.0.1 `
    --port 8000

# 步驟 2: 測試端點
curl http://localhost:8000/v1/models

# 步驟 3: 測試對話
curl http://localhost:8000/v1/chat/completions `
  -H "Content-Type: application/json" `
  -d '{
    "model": "Qwen/Qwen2-7B-Instruct",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# 步驟 4: 在 WhisperTrans 中設定
# LLM 提供商: 本地模型 (vLLM)
# API URL: http://localhost:8000/v1/chat/completions
# API Key: (留空)
# 模型名稱: Qwen/Qwen2-7B-Instruct

# 步驟 5: 點擊「測試連線」
```

### 2. OpenAI 完整測試

```powershell
# 步驟 1: 測試 API Key
curl https://api.openai.com/v1/models `
  -H "Authorization: Bearer sk-proj-YOUR_API_KEY"

# 步驟 2: 測試對話
curl https://api.openai.com/v1/chat/completions `
  -H "Content-Type: application/json" `
  -H "Authorization: Bearer sk-proj-YOUR_API_KEY" `
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# 步驟 3: 在 WhisperTrans 中設定
# LLM 提供商: OpenAI (GPT)
# API URL: https://api.openai.com/v1/chat/completions
# API Key: sk-proj-YOUR_API_KEY
# 模型名稱: gpt-3.5-turbo

# 步驟 4: 點擊「測試連線」
```

## 調試技巧

### 1. 啟用詳細日誌

在 `LLMTranslationService.cs` 中添加日誌：

```csharp
public async Task<string> TranslateAsync(string text, CancellationToken cancellationToken = default)
{
    System.Diagnostics.Debug.WriteLine($"開始翻譯: {text}");
    System.Diagnostics.Debug.WriteLine($"API URL: {_apiUrl}");
    System.Diagnostics.Debug.WriteLine($"模型: {_modelName}");
    
    // ... 翻譯邏輯 ...
    
    System.Diagnostics.Debug.WriteLine($"翻譯結果: {result}");
    return result;
}
```

### 2. 使用 Postman/Insomnia 測試

1. 建立新的 POST 請求
2. URL: `http://localhost:8000/v1/chat/completions`
3. Headers: `Content-Type: application/json`
4. Body:
```json
{
  "model": "Qwen/Qwen2-7B-Instruct",
  "messages": [
    {
      "role": "user",
      "content": "請將以下文字翻譯成繁體中文：Hello"
    }
  ]
}
```

### 3. 檢查 Windows Event Viewer

1. 開啟 Event Viewer
2. Windows Logs → Application
3. 查找 WhisperTrans 相關錯誤

### 4. 網路抓包

使用 Wireshark 或 Fiddler 查看實際的 HTTP 請求：

```
GET http://localhost:8000/v1/models
POST http://localhost:8000/v1/chat/completions
```

## 成功指標

### 連線測試成功

看到以下訊息表示成功：

```
? 連線成功！

本地模型 API 可以正常使用。

提供商: vllm
模型: Qwen/Qwen2-7B-Instruct
```

### Debug 輸出

Output 視窗應該顯示：

```
測試連線 - URL: http://localhost:8000/v1/chat/completions
測試連線 - 模型: Qwen/Qwen2-7B-Instruct
測試連線 - 提供商: vllm
開始翻譯: Hello
API URL: http://localhost:8000/v1/chat/completions
模型: Qwen/Qwen2-7B-Instruct
翻譯結果: 你好
```

## 快速檢查清單

### vLLM / Ollama (本地)

- [ ] vLLM/Ollama 伺服器已啟動
- [ ] URL 格式正確（包含 `/v1/`）
- [ ] 模型名稱與啟動時一致
- [ ] 端口未被占用
- [ ] 防火牆允許連線
- [ ] API Key 欄位可以留空

### OpenAI / Azure (雲端)

- [ ] API Key 正確且有效
- [ ] API Key 格式正確（sk-proj-...）
- [ ] 網路連線正常
- [ ] 未超過配額限制
- [ ] URL 格式正確
- [ ] 模型名稱正確

## 常見問題

### Q: 為什麼本地模型也顯示「請輸入 API Key」？

**A**: 這是之前的 bug，已修復。更新到最新版本後，本地模型不再強制要求 API Key。

### Q: 測試連線成功但實際翻譯失敗？

**A**: 可能是：
1. 翻譯文字太長
2. 模型不支援該語言
3. 回應格式不符

檢查 Debug 輸出以獲取詳細錯誤。

### Q: vLLM 啟動後馬上測試失敗？

**A**: 等待 1-2 分鐘讓模型完全載入。看到「Application startup complete」後再測試。

### Q: 可以同時使用多個翻譯服務嗎？

**A**: 目前只能同時使用一個。切換提供商後需要重新測試連線。

## 相關資源

- [vLLM 快速設定指南](./vLLM快速設定指南.md)
- [vLLM 本地模型整合說明](./vLLM本地模型整合說明.md)
- [LLM 翻譯功能說明](./LLM翻譯功能說明.md)
- [API Key 設定與儲存說明](./API Key設定與儲存說明.md)

## 回報問題

如果仍然無法解決，請提供：

1. WhisperTrans 版本
2. 選擇的 LLM 提供商
3. 完整的錯誤訊息
4. Debug 輸出（Output 視窗內容）
5. API URL 和模型名稱（不要包含 API Key）

---

**版本**: 1.2.4  
**最後更新**: 2024  
**狀態**: ? 已修復
